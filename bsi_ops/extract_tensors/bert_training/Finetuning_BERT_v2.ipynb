{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9f92f5",
   "metadata": {},
   "source": [
    "We are fine-tuning a pre-trained BERT for sentiment analysis on imdb dataset.\n",
    "Script written to run on any local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e284c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshajoshi/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5376d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9173099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):  # Adjust max_length as needed\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        tokenized_text = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',  # Pad to the specified max_length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Extract relevant tensors\n",
    "        input_ids = tokenized_text['input_ids'].squeeze()  # Remove the batch dimension\n",
    "        attention_mask = tokenized_text['attention_mask'].squeeze()\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0058c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample IMDb sentiment dataset\n",
    "texts = [\"This movie is great!\", \"I didn't like the ending.\"]\n",
    "labels = [1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "dataset = SentimentDataset(texts, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97616a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the BERT model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 2)  # 768 is the size of BERT's hidden layers\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5f09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentiment classifier\n",
    "classifier = SentimentClassifier(bert_model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f635f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10 completed\n"
     ]
    }
   ],
   "source": [
    "# Store Q, K, V vectors during training for all layers and attention heads\n",
    "Q_vectors = []\n",
    "K_vectors = []\n",
    "V_vectors = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    Q_vectors.append(module.query.weight.detach().cpu().numpy())\n",
    "    K_vectors.append(module.key.weight.detach().cpu().numpy())\n",
    "    V_vectors.append(module.value.weight.detach().cpu().numpy())\n",
    "\n",
    "# Register hooks for all attention heads in all layers\n",
    "for layer_idx in range(bert_model.config.num_hidden_layers):\n",
    "    for head_idx in range(bert_model.config.num_attention_heads):\n",
    "        attention_head = bert_model.encoder.layer[layer_idx].attention.self\n",
    "        attention_head.register_forward_hook(hook_fn)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for batch in DataLoader(dataset, batch_size=2, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        input_ids, attention_mask, labels = input_ids.to('cpu'), attention_mask.to('cpu'), labels.to('cpu')\n",
    "        logits = classifier(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{10} completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209e6ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ed6ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf52ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

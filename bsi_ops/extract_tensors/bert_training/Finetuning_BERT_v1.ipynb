{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5995a6c",
   "metadata": {},
   "source": [
    "We are fine-tuning a pre-trained BERT for sentiment analysis on imdb dataset.\n",
    "Script written to run on any local system.\n",
    "\n",
    "The Q, K, and V vectors are being extracted during the training loop of a BERT-based sentiment classifier. \n",
    "These vectors are associated with the attention mechanism in BERT and are used to compute attention scores.\n",
    "\n",
    "Q vectors: In the attention mechanism, the query vector is used to determine how much attention should be given to different positions in the input sequence. In BERT, each layer of the self-attention mechanism has its own set of query vectors.\n",
    "\n",
    "Key vectors: The key vector is used to determine the importance of different positions in the input sequence when computing attention scores.Like query vectors, each layer of the self-attention mechanism in BERT has its own set of key vectors.\n",
    "\n",
    "Value vectors: The value vector represents the information at different positions in the input sequence. Each layer of the self-attention mechanism in BERT has its own set of value vectors.\n",
    "\n",
    "In the code, a **hook function** (hook_fn) is registered for the **first attention head in the first layer** of the BERT model. This hook function is called during the forward pass, and it extracts the query, key, and value vectors for that specific attention head. The vectors are then appended to the corresponding lists (Q_vectors, K_vectors, and V_vectors). During each training iteration, these lists will be populated with the Q, K, and V vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f1156b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3a9fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):  # Adjust max_length as needed\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        tokenized_text = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',  # Pad to the specified max_length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Extract relevant tensors\n",
    "        input_ids = tokenized_text['input_ids'].squeeze()  # Remove the batch dimension\n",
    "        attention_mask = tokenized_text['attention_mask'].squeeze()\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fceac1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample IMDb sentiment dataset\n",
    "texts = [\"This movie is great!\", \"I didn't like the ending.\"]\n",
    "labels = [1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "dataset = SentimentDataset(texts, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f41ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the BERT model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 2)  # 768 is the size of BERT's hidden layers\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a05207f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentiment classifier\n",
    "classifier = SentimentClassifier(bert_model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965a49d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10 completed\n"
     ]
    }
   ],
   "source": [
    "# Store Q, K, V vectors during training\n",
    "Q_vectors = []\n",
    "K_vectors = []\n",
    "V_vectors = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    Q_vectors.append(module.query.weight.detach().cpu().numpy())\n",
    "    K_vectors.append(module.key.weight.detach().cpu().numpy())\n",
    "    V_vectors.append(module.value.weight.detach().cpu().numpy())\n",
    "\n",
    "# Register the hook for the first attention head in the first layer\n",
    "classifier.bert.encoder.layer[0].attention.self.register_forward_hook(hook_fn)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for batch in DataLoader(dataset, batch_size=2, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        input_ids, attention_mask, labels = input_ids.to('cpu'), attention_mask.to('cpu'), labels.to('cpu')\n",
    "        logits = classifier(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{10} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "740d9351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "441cc824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((V_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c7e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
